---
output:
  html_document: default
---
```{r intro, echo=FALSE, results="hide"}
#date & setting
date<-as.Date(Sys.time(	), format='%d%b%Y')
knitr::opts_chunk$set(echo=TRUE,
                      message=FALSE, 
                      comment = "", 
                      warning=FALSE) 
```
(Last updated: `r date`)

This is a course project for 'Practical Machine Learning'. It aims to predict the type/class of exercise in the test set, based on a model developed using the training set data. This report has the following sections:  

1. Data access and processing  
2. Model development, including justification, cross validation, and expected out of sample error.
3. Results of prediction model for 20 test cases   

__Note to the peer graders:__ I had lot of problems and questions for this assignment. I would love to learn from your experience/code, and would very much appreciate _specific_ suggestions to improve this assignment. Thank you in advance! 

####1. DATA PROCESSING 

#####1.1 Overall data structure 
First access the train and test data sets. 
```{r getdata, cache=TRUE}
#Get data
#train <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
#test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
setwd("C:/Users/YoonJoung Choi/Dropbox/2 R/2 R Coursera/8PracticalMachineLearning")
train <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
test <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

#Check data
nobstest<-nrow(test)
nobstrain<-nrow(train)
nvar<-ncol(train)
obspeople<-length(unique(train$user_name))
```
There are `r nobstest` observations in the test data, and `r nobstrain` in the train data set. The train data set has `r nvar` variables, including the outcome (i.e., __classe__, movement type), measured among `r obspeople` study participants. 

#####1.2 Outcome 
In __classe__, there are five different exercise types (A, B, C, D, and E). Class A is the correct movement, while the rest four classes are mistakes. They are:  
- exactly according to the specification (Class A)  
- throwing the elbows to the front (Class B)  
- lifting the dumbbell only halfway (Class C)  
- lowering the dumbbell only halfway (Class D)  
- throwing the hips to the front (Class E)  

Below is the distribution of the exercise pattern in the train data set (n=`r nobstrain`). 
```{r outcome, echo=FALSE}
table(train$classe) 
```

#####1.3 Covariates and data management 
There are so many potential covariates. Let's check them out briefly.  

First, Variables with lot of missing values?
```{r checkmissingTrain}
#check train data set first
nNA<-sapply(train, function(x) sum(is.na(x)))
var<-as.character(names(train))
tableTrain<-as.data.frame(cbind(var, nNA))
crosstab<-table(tableTrain$nNA)

table(tableTrain$nNA) 
```
In the training data, while `r crosstab[[1]]` variables have no missing values, the rest variables have missing values in most rows (19216 or more out of `r nobstrain` rows). We should drop those variables with lot of missing from the analysis. 

Or alternatively, drop variables with near zero variance. 
```{r nearzerovar}
library(caret)
train<-train[,-1]  #drop the first ID column
nearZeroVariance = nearZeroVar(train, saveMetrics=TRUE)
col_nearZeroVariance = nearZeroVar(train, saveMetrics=FALSE)
trainNEW = train[,-col_nearZeroVariance]
# also take care of "NA"
trainNEW[is.na(trainNEW)] = 0
dim(trainNEW)
```

Then, do the same processing for the test data. 
```{r}
#keep same columns 
testNEW<-test[,-1] #drop the first ID column
testNEW$classe = 0 #this is dummy column for data processing 
testNEW = testNEW[,colnames(trainNEW)]
#testNEW$problem_id = test$problem_id 
testNEW[is.na(testNEW)] = 0
dim(testNEW)
```

And, more cleaning to match class between train and test data.
```{r morecleaning}
testNEW$magnet_dumbbell_z = as.numeric(testNEW$magnet_dumbbell_z)
testNEW$magnet_forearm_y = as.numeric(testNEW$magnet_forearm_y  )
testNEW$magnet_forearm_z = as.numeric(testNEW$magnet_forearm_z)
testNEW$classe  = as.factor(testNEW$classe)

testframe = data.frame(sapply(testNEW,class),sapply(trainNEW,class))
testframe[which(testframe$sapply.testNEW..class. != testframe$sapply.trainNEW..class.),]
```

Finally, ensure same levels for factor variables between train and test data sets. This seems to cause a problem, unless fixed. 
```{r factorlevel}
common <- intersect(names(trainNEW), names(testNEW)) 

for (p in common) { 
  if (class(trainNEW[[p]]) == "factor") { 
    levels(testNEW[[p]]) <- levels(trainNEW[[p]]) 
  } 
}
```

####2. MODEL DEVELOPMENT

#####2.1 Model selection strategy 
There are a few things to consider to choose a modeling approach.  
- The outcome is a categorical variable, thus a non-linear function is need.  
- There are over 90 covariates, thus methods that can handle a large number of covariates would be helpful (unless we summarize/reduce covariates by using PCA, for example. _(But, a quetion: is it correct that PCA is used for linear models mainly?)_  
- Observations are potentially correlated within each person. If that's the case and modeling does not address that, estimates may be biased. _(But, another question: I really don't know how to handle that issue. would apprecaite any suggestions on this)_  

I chose to use Random forest, partially because of its built-in cross validation function, since it basically combines multiple decision trees (from bagging) to determine the final output. 

Before moving on, by the way, there are so many observations (`r nobstrain`!), and it takes way too long to run even one model. So, let's randomly select 60% of train data and use that for model fitting train data __(trainNEWsub)__. The rest will be used for cross-validation __(trainNEWvalidation)__, which will be used to assess out of sample error.  
```{r subsample, results="hide"}
set.seed(1234)
library(caret)

inSubsample <- createDataPartition(y=trainNEW$classe,
                              p=0.60, list=FALSE)
trainNEWsub <- trainNEW[inSubsample,]
trainNEWvalidation <- trainNEW[-inSubsample,]
```

#####2.2 Decision Tree - just checking
Before trying the random forest, let's see a decisoin tree. 
```{r model, cache=TRUE}
library(rpart)
tree1 <- rpart(classe ~ ., data=trainNEWsub, method="class")
pred_tree1 <- predict(tree1, trainNEWvalidation, type = "class")
confusionMatrix(pred_tree1, trainNEWvalidation$classe)
```

#####2.3 Random Forest 
Now, let's see random forest models. See how accuracy in validation dataset is much higher than that in a decision tree above.  
```{r forest, cache=TRUE}
library(randomForest)
forest1 <- randomForest(classe ~. , data=trainNEWsub)
pred_forest1 <- predict(forest1, trainNEWvalidation, type = "class")
confusionMatrix(pred_forest1, trainNEWvalidation$classe)
```

Then, refine the model by changing the number of trees and the number of covariates (i.e., the number of variables randomly sampled at each stage).  
```{r model_refine, cache=TRUE}
set.seed(1234)

# set train control 
trControl <- trainControl(method = "cv",
    number = 5,
    search = "grid")

# set mtry to try
tuneGrid <- expand.grid(.mtry = c(7, 14, 21, 28))

# three different ntree: 50, 100, 200
model1 <- train(classe~., trainNEWsub, method = "rf", metric = "Accuracy", 
                trControl = trControl, importance = TRUE, tuneGrid = tuneGrid,
                ntree = 50)

model2 <- train(classe~., trainNEWsub, method = "rf", metric = "Accuracy", 
                trControl = trControl, importance = TRUE, tuneGrid = tuneGrid,
                ntree = 100)

model3 <- train(classe~., trainNEWsub, method = "rf", metric = "Accuracy", 
                trControl = trControl, importance = TRUE, tuneGrid = tuneGrid,
                ntree = 200)

model4 <- train(classe~., trainNEWsub, method = "rf", metric = "Accuracy", 
                trControl = trControl, importance = TRUE, tuneGrid = tuneGrid,
                ntree = 500)
```

#####2.2 Assess accuracy and select a final model 
Check accuracy using the validation data (i.e., the rest 40% of the train data).
```{r accuracy, cache=TRUE}

pred1 <-predict(model1, trainNEWvalidation)
pred2 <-predict(model2, trainNEWvalidation)
pred3 <-predict(model3, trainNEWvalidation)
pred4 <-predict(model4, trainNEWvalidation)

confusionMatrix(pred1, trainNEWvalidation$classe)
confusionMatrix(pred2, trainNEWvalidation$classe)
confusionMatrix(pred3, trainNEWvalidation$classe)
confusionMatrix(pred4, trainNEWvalidation$classe)
```
Above confusion matrices, based on evaluation of model performance on validation data, suggest that model 4 may be most appropriate. Expected out of sample error is very low (<0.1%). __But, I'm worried about over fitting or some other problems. How can it have near 100% accuracy with the validation data?!__ Anyhow, move along...

####3. PREDICTION RESULTS 

One more data management for the test data.
```{r}
testNEW <- rbind(trainNEWsub[1,],testNEW)
testNEW <- testNEW[-1,]
```

Finally, prediction!
```{r predict}
predict<-predict(model4, testNEW)
```


#### REFERENCE 
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz681eWk8h6

